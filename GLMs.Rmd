---
title: "A Short Introduction to Generalized Linear Models"
author: \emph{Katie Frank}
date: \emph{2/12/2020}
output: 
  pdf_document:
    fig_caption: true
    includes:
      in_header: preamble-latex.tex
fontsize: 11pt
urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2); theme_set(theme_bw())
```

# Overview

In this document, I introduce a special class of statistical models known as generalized linear models, focusing on their formulation and estimation. Towards the end of the document, I work through an example of fitting a generalized linear model to survival data.

# Exponential family

The **exponential family** of distributions are an important class of distributions in statistics. For a random variable $Y$ parameterized by $\th$ and $\phi$, the probability density function (pdf) of $Y$ belongs to the exponential family if it can be expressed in the form

\begin{equation}
f(y; \th, \phi) = \exp\l[\frac{y\th - b(\th)}{a(\phi)} + c(y, \phi)\r],
\end{equation}

where $a(\phi), b(\th)$, and $c(y, \phi)$ are known functions. Typically $a(\phi)$ has the form

$$a(\phi) = \frac{\phi}{p},$$

where $p$ is a set *prior weight*; in most cases $p = 1$. The parameter $\th$ is known as the *canonical parameter* and is usually the parameter of interest, while $\phi$ is a nuisance parameter that affects the variance. If $Y$ is assumed to be generated by a distribution in the exponential family, then

\begin{align}
\text{E}[Y] &= \mu = b^{\prime}(\th) \label{eq:mean} \\
\text{Var}(Y) &= \si^2 = b^{\prime\prime}(\th)a(\phi), \label{eq:var}
\end{align}

where $b^{\prime}(\th)$ and $b^{\prime\prime}(\th)$ denote the first and second derivatives of $b(\th)$. Many popular distributions, such as the normal, Bernoulli, and Poisson, are members of the exponential family. 

\newpage

## Exponential example

The exponential distribution is a member of the exponential family. Let $Y \sim \text{Exp}(\la)$. The pdf of $Y$ is

$$f(y; \la) = \la e^{-\la y}, \quad y > 0,$$

where $\la > 0$ is known as the *rate parameter* of the distribution. This expression can be rewritten as

$$f(y; \la) = \exp\l[-\la y + \log{\la}\r].$$

Here, $\th = -\la$. Thus, $b(\th) = -\log(-\th)$. Also, $a(\phi) = \phi = 1$ and $c(y, \phi) = 0$. From equations (\ref{eq:mean}) and (\ref{eq:var}), the mean and variance are

\begin{align*}
\text{E}[Y] &= b^{\prime}(\th) = - \frac{1}{\th} = \frac{1}{\la} \\
\text{Var}(Y) &= b^{\prime\prime}(\th)a(\phi) = \frac{1}{\th^2} = \frac{1}{\la^2}.
\end{align*}

The exponential distribution is used to model the time until an event occurs, otherwise known as the waiting time. For instance, the time to failure of a specific brand of light bulb could be well-modeled by an exponential distribution, where $\la$ controls the rate of failure. A large value for $\la$ indicates that the bulbs tend to fail more quickly, while a small $\la$ implies the opposite. Figure \ref{fig:exps} illustrates this idea.

```{r exps, echo=FALSE,warning=FALSE, out.width="80%", fig.align = "center", fig.cap = "\\label{fig:exps}Probability density function of Exp(1) and Exp(2) in black and red, respectively.", out.width="80%"}
la1 <- expression(paste(lambda, " = 1"))
la2 <- expression(paste(lambda, " = 2"))

ggplot(data.frame(time = c(0, 5)), aes(time)) +
  stat_function(fun = dexp, args = list(rate = 1), show.legend = TRUE) +
  stat_function(fun = dexp, args = list(rate = 2), color = "red") +
  annotate("text", x = 2, y = .4, label = la1, size = 6) +
  annotate("text", x = .7, y = 1.25, label = la2, size = 6, color = "red") +
  xlab("Time to failure") +
  ylab("Probability density")
```

\newpage

# Generalized linear models

The generalized linear model (GLM) can be thought of as an extension of linear regression to non-normal response variables. In a GLM, the *linear predictor* $\eta_i = \matr{x}_i^{\prime}\bm{\be}$ is related to the response variable $y_i$ through a *link function*, denoted $g$. The function $g$ must be monotone increasing and differentiable.

In the linear predictor,

- $\matr{x}_i$ is a $(p \times 1)$ vector of explanatory/predictor variables for the $i$th object/subject.
$$\matr{x}_i = \begin{bmatrix} 
x_{i1} \\ 
\vdots \\
x_{ip} 
\end{bmatrix} \quad \text{so} \quad \matr{x}_i^{\prime} = 
\begin{bmatrix}
x_{i1} & \cdots & x_{ip}
\end{bmatrix}$$
The vector $\matr{x}_i^{\prime}$ constitutes the $i$th row of the design matrix $\matr{X}$.
$$\matr{X} = \begin{bmatrix}
\matr{x}_1^{\prime} \\
\vdots \\
\matr{x}_n^{\prime} \\
\end{bmatrix} = \begin{bmatrix}
x_{11} & \cdots & x_{1p} \\
\vdots & & \vdots \\
x_{n1} & \cdots & x_{np}
\end{bmatrix}$$
In usual cases, $n$ (the number of objects/subjects) is much larger than $p$.\vskip .075in
- $\bm{\be}$ is the $(p \times 1)$ vector of model coefficients to be estimated.
$$\bm{\be} = \begin{bmatrix}
\be_1 \\
\vdots \\
\be_p
\end{bmatrix}$$


Given predictors $\matr{x}_i$, the response variable $y_i$ is assumed to be generated from a distribution in the exponential family with mean $\mu_i$. The expected value of $y_i$ given explanatory variables $\matr{x}_i$ is defined

$$\text{E}[y_i|\matr{x}_i] = \mu_i = g^{-1}(\eta_i),$$

where $\eta_i = \matr{x}_i^{\prime}\bm{\be}$ is the linear predictor and $g$ is the link function.

A GLM consists of three components.

1. *Random component* - the probability distribution of the response variable $y_i|\matr{x}_i$ (e.g.,  $y_i|\matr{x}_i \sim \text{N}(\mu_i, \si^2)$  in linear regression and $y_i|\matr{x}_i \sim \text{Bern}(\pi_i)$ in logistic regression).
2. *Systematic component* - the linear combination of explanatory variables used to create the linear predictor (i.e., $\eta_i = \be_0 + \be_1x_{i1} + \cdots + \be_px_{ip}$).
3. *Link function* - the connection between the random and systematic components. it says how $\mu_i$, the mean of $y_i|\matr{x}_i$, is related to the linear predictor $\eta_i = \matr{x}_i^{\prime}\bm{\be}$.

If the link function connects the linear predictor $\eta_i$ and the canonical parameter $\th_i$ such that $\eta_i = \th_i$, then the link function is *canonical*.

## Estimation

Estimation of the model parameters $\bm{\be} = (\be_0, \dots, \be_p )^{\prime}$ in a GLM are obtained through maximum likelihood using an iteratively reweighted least-squares (IRLS) algorithm. Here is how it works. Start off with an initial estimate of the parameters $\hat{\bm{\be}}$.

1. Calculate the $n\times 1$ *working response* vector $\matr{z}$. The $i$th element of $\matr{z}$ is 
$$z_i = \hat{\eta}_i + (y_i - \hat{\mu}_i)\l(\frac{\partial \eta_i}{\partial \mu_i}\r),$$
where $\frac{\partial\eta_i}{\partial \mu_i}$ is evaluated at $\hat{\bm{\be}}$.\vskip .075in
2. Calculate the $n\times n$ diagnonal *weight matrix* $\matr{W}$, where the $i$th element is
$$w_{ii} = \frac{1}{\text{Var}(Y_i|\matr{x}_i)}\l(\frac{\partial \mu_i}{\partial \eta_i}\r)^2.$$
3. Calculate the (new) weighted least-squares estimate
\begin{equation}
\hat{\bm{\be}} = (\matr{X}^{\prime}\matr{WX})^{-1}\matr{X}^{\prime}\matr{W}\matr{z}.\label{eq:irls}
\end{equation}

Repeat this procedure until the absolute difference between successive approximations $|\hat{\bm{\be}}^{(m)} - \hat{\bm{\be}}^{(m-1)}|$ is smaller than some tolerance level. Equation \ref{eq:irls} is a convenient way of expressing the weighted least-squares estimate. It is equivalent to

\begin{equation}
\hat{\bm{\be}}^{(m)} = \hat{\bm{\be}}^{(m-1)} + \mathcal{I}\l(\hat{\bm{\be}}^{(m-1)}\r)^{-1}U(\hat{\bm{\be}}^{(m-1)}),
\end{equation}

where $\mathcal{I}(\hat{\bm{\be}}^{(m-1)})$ is the expected information matrix and $U(\hat{\bm{\be}}^{(m-1)})$ is the score function.

\newpage

# GLM example with survival data

This example comes from Exercise 4.2 on p. 77-78 of [Dobson and Barnett (2018)](https://www.crcpress.com/An-Introduction-to-Generalized-Linear-Models/Dobson-Barnett/p/book/9781138741515). The column `surv_weeks` in the truncated data frame below contains survival times (in weeks) from diagnosis to death for seventeen patients with leukemia. The `log_wbc` column denotes each patient's $\log_{10}$(initial white blood cell count).

\vskip .1in

```{r}
dat <- read.table("leukemia.txt", header = TRUE, sep = "\t")
head(dat)
```

\vskip .1in

If we plot `surv_time` against `log_wbc`, we see that survival time decreases approximately exponentially as initial log white blood cell (WBC) count increases. Thus, we assume $Y_i|x_i \sim \text{Exp}(\la_i)$, for $i = 1, \dots, n$. Also, we assume the $Y_i$'s are independent.

\vskip .1in

```{r, out.width="80%", fig.align="center"}
library(ggplot2); theme_set(theme_bw())

ggplot(dat, aes(log_wbc, surv_weeks)) +
  geom_point() +
  xlab(expression(log[10]("initial white blood cell count"))) +
  ylab("Survival time (weeks)")
```

\vskip .1in

If we want to model the survival times based on $\log_{10}$(initial WBC count) one possible specification is

$$\text{E}[y_i|x_i] = \mu_i = \exp(\be_1 + \be_2x_i),$$

where $y_i$ is the survival time and $x_i$ is the log WBC count. Here, the $\exp()$ in $\exp(\be_1 + \be_2x_i)$ is the inverse link function. So, we are working with the log link: $g(\mu_i) = \log(\mu_i)$. Unlike the canonical link, which is $g(\mu_i) - \mu_i^{-1}$, the log link ensures that $\mu_i$ is non-negative for all values of the parameters and $x_i$.

In our specification for the conditional expectation of $Y_i$, we have

\begin{align*}
\text{E}[Y_i|x_i] = \mu_i &= \exp(\be_1 + \be_2x_i) \\
  &= \exp(\matr{x}_i^{\prime}\bm{\be}),
\end{align*}

where $\bm{\be} = \begin{bmatrix}
\be_1 \\
\be_2 \\
\end{bmatrix}$ and $\matr{x}_i = \begin{bmatrix}
1 \\
x_i \\
\end{bmatrix}$ for $i = 1, \dots, n$.

The link function is

$$\log(\mu_i) = \matr{x}_i^{\prime}\bm{\be} = \eta_i.$$

Thus, $\frac{\partial \mu_i}{\partial \eta_i} = \exp(\eta_i)$ and $\frac{\partial \eta_i}{\partial \mu_i} = \frac{1}{\mu_i}$. 

The $i$th element of the $n\times 1$ working response vector $\matr{z}$ is

\begin{align*}
z_i &= \eta_i + (y_i - \mu_i)\l(\frac{\partial \eta_i}{\partial \mu_i}\r) \\
  &= \log(\mu_i) + (y_i - \mu_i)\l(\frac{1}{\mu_i}\r) \\
  &= \matr{x}_i^{\prime}\bm{\be} + (y_i - \exp(\matr{x}_i^{\prime}\bm{\be}))\frac{1}{\exp(\matr{x}_i^{\prime}\bm{\be})} \\
  &= \matr{x}_i^{\prime}\bm{\be} + \frac{y_i}{\exp(\matr{x}_i^{\prime}\bm{\be})} - 1.
\end{align*}

The $i$th weight in the $n\times n$ diagonal weight matrix $\matr{W}$ is 

\begin{align*}
w_{ii} &= \frac{1}{\text{Var}(Y_i|\matr{x}_i)}\l(\frac{\partial \mu_i}{\partial \eta_i}\r)^2 \\
  &= \la_i^2\l[\exp(\eta_i)\r]^2 \\
  &= \la_i^2\l[\exp(\log(\mu_i))\r]^2 \\
  &= \la_i^2 \mu_i^2 \\
  &= 1.
\end{align*}

We now have everything we need to find the maximum likelihood estimates (MLEs). In the code below, I start the IRLS procedure with $\hat{\be}_1 = 10$ and $\hat{\be}_2 = -1$.

\vskip .1in

```{r}
irls_exp <- function(x, y, be1, be2, max_iter = 100, tol = 1e-5) {
  X <- matrix(c(rep(1, length(x)), x), ncol = 2)
  be <- matrix(c(be1, be2), ncol = 1)
  be_prev <- be
  
  for (iter in 1:max_iter) {
    z <- X %*% be + y / (exp(X %*% be)) - 1
    cov_mat <- solve(t(X) %*% X) 
    be <- cov_mat %*% t(X) %*% z
    
    if (abs(be[1] - be_prev[1]) < tol && abs(be[2] - be_prev[2]) < tol) {
      break
    } else {
      be_prev <- be
    }
  }
  list(be = be, cov_mat = cov_mat, num_iter = iter)
}

irls_exp(dat$log_wbc, dat$surv_weeks, 10, -1)
```

\vskip .1in

For our given tolerance level of \num{1e-5}, it took 8 iterations to obtain the MLEs. These estimates are $\hat{\be_1} = 8.4775$ and $\hat{\be_2} = -1.1093$, and their estimated covariance matrix is given in `cov_mat`. The estimated standard error for $\hat{\be}_1$ is $\sqrt{2.7384} = 1.6548$ and for $\hat{\be}_2$ it is $\sqrt{0.1597} = 0.3997$.

An approximate $95\%$ Wald confidence interval for $\be_2$ is

$$-1.1093 \pm 1.96(0.3997) = (-1.8927, -0.3259).$$

Interpretation: for every 1 unit increase in $\log_{10}$(initial WBC count) the average survival time multiplies by $\exp(\hat{\be}_2) = 0.3298$. In short, it decreases.

Instead of creating our own function to fit the model, we could have used \textsf{R}'s `glm()` function.

\vskip .1in

```{r}
mod <- glm(surv_weeks ~ log_wbc, family = Gamma(link = "log"), data = dat)
summary(mod)
```

\vskip .1in

Observe that the estimated standard errors of $\hat{\be}_1$ and $\hat{\be}_2$ differ from what we obtained from our function `irls_exp()`. This is because in the call to `glm()` the `family = Gamma()` component fits the more general gamma distribution rather than the exponential distribution. The gamma distribution is parameterized by two parameters (mean and dispersion), which is why we see the line "Dispersion parameter for Gamma family taken to be 0.9388638". Since the exponential distribution is parameterized by a single parameter (mean), the dispersion is not estimated: it is just 1.

\newpage

If we specify `dispersion = 1` in the call to `summary()`, then the standard error estimates will correspond to the values we obtained from `irls_exp()`.

\vskip .1in

```{r}
summary(mod, dispersion = 1)
```

\newpage

We can plot the fitted curve for our model to the data.

\vskip .1in

```{r, out.width="80%", fig.align="center"}
ggplot(dat, aes(log_wbc, surv_weeks)) +
  geom_point() + 
  geom_smooth(method = "glm", formula = y ~ x, se = FALSE, 
    method.args = list(family = Gamma(link = "log"))) + 
  xlab(expression(log[10]("initial white blood cell count"))) +
  ylab("Survival time (weeks)")
```

\vskip .1in

To assess the adequacy of the model fit, we can compare the observed values $y_i$ and the fitted values $\hat{y}_i = \hat{\mu}_i = \exp(\hat{\be}_1 + \hat{\be}_2x_i)$ using standardized residuals, denoted $r_i$.

\begin{align*}
r_i &= \frac{(y_i - \hat{y}_i)}{\hat{\si}_i} \\
  &= \frac{(y_i - \hat{y}_i)}{\sqrt{\text{Var}(Y_i|\matr{x}_i)}} \\
  &= \frac{(y_i - \hat{y}_i)}{\sqrt{\frac{1}{\hat{\la}_i^2}}} \\
  &= \frac{(y_i - \hat{y}_i)}{\frac{1}{\hat{\la}_i}} \\
  &= \frac{(y_i - \hat{y}_i)}{\hat{y}_i}.
\end{align*}

\newpage

In the plot of the standardized residuals versus the fitted values, observe that all but one of the residuals are relatively close to 0. Thus, the model appears to fit the data well. The one outlying residual at $r = 2.467$ corresponds to the observation $(y = 65, x = 5)$.

\vskip .1in

```{r, out.width="80%", fig.align="center"}
r <- (dat$surv_weeks - mod$fitted.values) / mod$fitted.values

# plot of standardized residuals vs. fitted values
ggplot(data.frame(fitted_values = mod$fitted.values, residual = r), 
  aes(fitted_values, residual)) +
  geom_point() + 
  geom_hline(yintercept = 0, linetype = "dashed") +
  ylim(c(-2.75, 2.75)) + 
  xlab("Fitted value") +
  ylab("Residual")
```

